{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October\n",
    "## Loan Default prediction\n",
    "##### Find default loan clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "Imagine a car loan company specialized in proviing car loans to retailed clients. Since retailed clients required authorization or regulation to operate in the financial markets, the company need to decide wisely if the client is eligible for a loan. That means that the company will needs to secure its portfolio and facilitate descision processes. In the same scope, the goal of the current case study is to build a system that detects if a client is loan default, based on client data. More precisely, a machine learning model will be trained on client data aiming to learn how clients can be classified as default or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a binary classification problem which can be tackled using machine learning or deap learning algorithms. We can conclude the following points:\n",
    "- Descision on loan default \n",
    "- Detect loan default clients\n",
    "- Binary classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the data will be discussed and explored. Some interesting findings regarding data distributions and structures will be visualized. The code block below import the required libraries, loads the data and performs the preprocess state. We performed all the basic steps of data manipulation and cleaning:\n",
    "\n",
    "- Check for dublicated clients\n",
    "- Removing NaN, +inf, -inf\n",
    "- Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_manager import DataManager\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize DataManager class\n",
    "Data manager is responsible for the data manipulation, the preprocess and the cleaning. The next blocks of code provide info about the raw data in the car_loan_trainset dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager = DataManager(path_to_data='../data_folder/car_loan_trainset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Initial data shape: {data_manager.data.shape}')\n",
    "print('Data columns:', data_manager.data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager.data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess step of data manager is performing the essential preprocessing steps in the data and returns the columns will be fit in the model. \n",
    "#### Excluded columns and records\n",
    "- All NA and inf records\n",
    "- All columns with ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = data_manager.get_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction\n",
    "We tried to extract some information from some specific variables from the dataset by implementing ratios.\n",
    "##### Fetarue extraion ratios\n",
    "- Overdue to active ratio = $\\frac{ratio\\_overdue}{main\\_active\\_loan + sub\\_active\\_loan}$ \n",
    "\n",
    "\n",
    "- Overdue_to_total_ratio = $\\frac{total\\_overdue}{total\\_account\\_loan}$\n",
    "\n",
    "\n",
    "\n",
    "- Monthly_payment_to_outstanding_ratio = $\\frac{total\\_monthly\\_payment}{total\\_outstanding\\_loan}$\n",
    "\n",
    "\n",
    "\n",
    "- Outstanding_to_disburse_raio = $\\frac{total\\_outstanding_loan}{total\\_disbursed\\_loan}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess step also performs some logarithmic transformation on skewd variables:\n",
    "- Total_outstanding_loan\n",
    "- Total_disbursed_loan\n",
    "- Total_monthly_payment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing step we binarize the variable age into: $age = \\begin{cases} 1, & \\text{if  age } >= 30\\\\\n",
    "0, & \\text{if  age } < 30 \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable employment type is a categorical variable with 3 levels so it is turned into one hot encoding, which means a $n\\times3$ matrix with zeros and $1$ in the position which coresponds to the employment type level. In this way the model can handle the categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewed variables\n",
    "The following tow plots visualize the raw total_outstanding_loan before and after the log transformation. It can be easily observed that initialy is very skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale = 2, rc={'axes.facecolor': 'white', 'figure.facecolor': 'white'})\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = data_manager.data.sample(350)\n",
    "sns.displot(data=data_subset, x=\"total_outstanding_loan\", hue=\"loan_default\", height=10, aspect=2).set(title='Raw total_outstanding_loan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_subset = data.sample(350)\n",
    "sns.displot(data=data_subset, x=\"total_outstanding_loan\", hue=\"loan_default\", height=10, aspect=2).set(title='Log total_outstanding_loan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age\n",
    "The age histogram illustrates the average age of a loan. We can see the we cannot observe any pattern or a tendency that classifies the two types of clients into default or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_subset = data[data['average_age'] < 50]\n",
    "sns.displot(data=data_subset, x=\"average_age\", hue=\"loan_default\", height=10, aspect=2).set(title='Average Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=data, x=\"loan_to_asset_ratio\", hue=\"loan_default\", height=10, aspect=2).set(title='Loan to asset ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot is a box plot for the loan to asset ratio. Data cannot be easily separted between the two classes because the distirbutions are overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.boxplot(data=data, x=\"loan_default\", y='loan_to_asset_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=data, x=\"credit_history\", hue=\"loan_default\", height=10, aspect=2).set(title='Credit history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=data, x=\"Credit_level\", hue=\"loan_default\", height=10, aspect=2).set(title='Credit level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_subset = data[data['overdue_to_active_ratio'] < 2]\n",
    "sns.displot(data=data_subset, x=\"overdue_to_active_ratio\", hue=\"loan_default\", height=10, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=data_subset, x=\"overdue_to_total_ratio\", hue=\"loan_default\", height=10, aspect=2).set(title='Overdue to total ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings from exploration\n",
    "The data are imbalanced, the loan default clients are less than the non ones. We can also conclude from the plots and the exploration that the features are not separated well in the two classes. That means that the classifier will not perform highly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Proportion of class 1:', len(data.loan_default[data.loan_default==1])/data.loan_default.size)\n",
    "print('Proportion of class 0:', len(data.loan_default[data.loan_default==0])/data.loan_default.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Because we have high variant values in the variables of the dataset, we perform a pre-trainning step of scaling the data using a MinMax scaler which transforms the data in a range of $X_i \\in [-2,2]$. We split the data into train and validation set (test set) using a proportion of $20\\%$ for the testset. The model is an Multilayer perceptron with one hidden layer and 100 nodes. The model was trained for 300 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metrics we used are:\n",
    "- Precission\n",
    "- Recall\n",
    "- F1 score\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the endpoint for training the model using FastAPI. We have two methods deployed for handling the imbalance classes in the dataset:\n",
    "- Undersampling\n",
    "- Oversampling\n",
    "Those two can be used for training the model. The request /train needs the model name and the method for handling imbalance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "oversampling = \"False\"\n",
    "model = \"MLP\"\n",
    "request_url = f\"http://127.0.0.1:8000/train/{model}/oversampling/{oversampling}\"\n",
    "response = requests.post(request_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Model\":\"MLP\",\"Accuracy score\":0.5438149025323374,\"Precision score\":0.2290448343079922,\"Recall score\":0.6628205128205128,\"F1 score\":0.340445146845779}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "It is obvious that the performance of the model is the best according to accuracy and F1 score. The interesting findins though are that the recall score is high which means that the amount of false negative predictions are low. This is interesting because in the specific problem the task is to build a system which detects the loan default clients. So, the system needs to have low number of falsely negative classified clients which means that if a client is class=1 (loan_deafult) it is more possible to be detected and classified correctly. Unfortunately the ratio between precision recall is not so balance since the precision is very low, which means that the false positives are relatively low. The accuracy in this case is not so informative regarding the performance of the binary classifier since the dataset is very imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "token = base64.b64encode(b'October:123').decode('utf-8')\n",
    "body = {\n",
    "  \"Driving_flag\": 1,\n",
    "  \"last_six_months_new_loan_no\": 0,\n",
    "  \"last_six_month_defaulted_no\": 12,\n",
    "  \"average_age\": 40,\n",
    "  \"credit_history\": 0,\n",
    "  \"loan_to_asset_ratio\": 0.65,\n",
    "  \"total_outstanding_loan\": 0,\n",
    "  \"total_disbursed_loan\": 0,\n",
    "  \"total_monthly_payment\": 123,\n",
    "  \"active_to_inactive_act_ratio\": 0.43,\n",
    "  \"Credit_level\": 2,\n",
    "  \"age\": 28,\n",
    "  \"loan_default\": 1,\n",
    "  \"employment_type\": 2,\n",
    "  \"total_overdue_no\": 1000,\n",
    "  \"main_account_active_loan_no\": 0,\n",
    "  \"total_account_loan_no\": 0,\n",
    "  \"sub_account_active_loan_no\": 0\n",
    "}\n",
    "response = requests.post('http://127.0.0.1:8000/predict/MLP', headers={\"Authorization\": f\"Basic {token}\"}, json=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"prediction_outcome_class\":0,\"username\":\"October\"}'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work\n",
    "- Extract more features which classify better the data\n",
    "- Use feature selection techinques e.g. Elastinet\n",
    "- Experimentation with various scaling-feature methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
